import{_ as r,E as s,c as a,m as e,a as o,J as l,w as h,V as t,o as n}from"./chunks/framework.ecnLIU6R.js";const x=JSON.parse('{"title":"Large language models won’t replace the programmers tomorrow","description":"","frontmatter":{"outline":[2,4]},"headers":[],"relativePath":"docs/llms-replace-humans.md","filePath":"docs/llms-replace-humans.md"}'),d={name:"docs/llms-replace-humans.md"},c=t('<h1 id="large-language-models-won-t-replace-the-programmers-tomorrow" tabindex="-1">Large language models won’t replace the programmers tomorrow <a class="header-anchor" href="#large-language-models-won-t-replace-the-programmers-tomorrow" aria-label="Permalink to &quot;Large language models won’t replace the programmers tomorrow&quot;">​</a></h1><p><a href="https://commons.wikimedia.org/wiki/File:IBM_150_Extra_Engineers_1951.jpg" target="_blank" rel="noreferrer"><img src="https://upload.wikimedia.org/wikipedia/commons/b/b8/IBM_150_Extra_Engineers_1951.jpg" alt=""></a></p><p>Hi!</p><h2 id="the-fickle-genie" tabindex="-1">The fickle genie <a class="header-anchor" href="#the-fickle-genie" aria-label="Permalink to &quot;The fickle genie&quot;">​</a></h2><p>Odds are, you&#39;re fuming with anger having clicked on an obvious clickbait, trying to capitalise on... shall we say... the mass disaster of the day. After all, there <em>were</em> many who underestimated the development of ML models, and there are some skeptical to this day. Some of the artists who were laughing years ago, are protesting today, boycotting the use of statistical models and generative &quot;art&quot;. Still, there is a <strong>but</strong>.</p><p>LLMs generate numbers. Numbers can be text tokens, can be pixels, sound, 3d models, or they could be machine instructions including code and outputs of some compilers and systems like verilog. The efficacy of LLMs is predicated on the tooling. And, you see, programming is not just about writing code and knowing the proper names of certain things. Otherwise, compilers would have replaced programmers long ago. Oh no, programming is a language skill, coupled with some real-world experience, that would allow a programmer to spot a mistake in the technical specifications of the code that they are asked to write; more often than not the program does exactly what you asked it to do, but just like a fickle genie, it doesn&#39;t correct for &quot;common-sense&quot;.</p><h3 id="full-self-driving-still-needs-a-driver" tabindex="-1">Full self-driving still needs a driver <a class="header-anchor" href="#full-self-driving-still-needs-a-driver" aria-label="Permalink to &quot;Full self-driving still needs a driver&quot;">​</a></h3><p>The guiding principle of transformers and transformer-like models is next token prediction. Imagine, you remember a lot and speak long sentences without thinking much. That’ll be rather similar to the results of the process that most LLMs utilise to generate the &quot;magical&quot; results. These methods can be weaponised to search for text, or they could be used for word substitution.</p><p>But, I argue, that&#39;s still far from &quot;solving a problem&quot;; it&#39;s not about building an action plan, executing the actions and tracking their flow, just stochastic parroting. It can mimic structures that resemble, or rather allow us to project these qualities, when in fact the underlying model has no concept/understanding of what a plan would even mean. Some may disagree and point to the concept of multi-headed attention<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup> as a way of imitating a thinking process. I disagree, and as I argued earlier, this is a function of observer projection and not genuine procedural conceptualisation.</p><p>You indeed can ask an LLM to criticize your code or its documentation. A (relatively) fresh LLM that was trained on language code and documentation might even be helpful, but not to anyone beyond the first stages of learning, or deeply invested in seeing competence. You can query it for debugger interactions and it might help. There&#39;s also a chance that it is hallucinating, and it might lead you down a wild goose chace, though if it does, you can simply repeat the request. The system still requires a human to oversee the operation of the LLM. It&#39;s no auto-pilot if you still need a human to take over the driving, if the &quot;full-self-driving&quot; does something so stupid that it could have been caught algorithmically.</p><p>So LLMs are not as thorough, and cannot, at present, fully automate the existence of a human behind a screen doing the legwork. It didn&#39;t stop a marketing misnomer LLMs -- &quot;AIs&quot; to have become popular. Let us suppose that all thinking is algorithmic (which is not a falsifiable theory), and also suppose that a thinking machine might think without having a model of the world explicitly, but implicit in its weights. Can LLMs answer questions on programming in languages that haven&#39;t been invented at the time of their training? Will they be as accurate? Can they work off of limited training data, as most humans do? It still doesn&#39;t cut it.</p><p>Rather than regurgitating our earlier discussion in the old <a href="https://odysee.com/@CyberLounge:a/will-ai-ever-replace-human-programmers-part-3:c" target="_blank" rel="noreferrer">CyberLounge</a>, we will focus on newer developments. The conclusion hasn&#39;t changed, but the inputs might fool you into thinking that it did.</p><p>So without further ado.</p><h3 id="next-token-prediction-is-not-thinking" tabindex="-1">Next token prediction is not thinking <a class="header-anchor" href="#next-token-prediction-is-not-thinking" aria-label="Permalink to &quot;Next token prediction is not thinking&quot;">​</a></h3><p>To find out why, let’s get meta: think about how we think. &quot;I’d put this word here, because it looks relatively appropriate&quot;? Maybe it&#39;s something I do occasionally when writing documentation in a foreign language, but not routinely. Our way of solving problems is spread over several distinct layers of abstraction, starting with concepts and ending in language. We descend the layers of abstraction from the conceptual stage, where we have plans and actions, concepts and inferences, we have what I would argue, thought in its purest and most abstract form. At this stage we can think in terms of vague pictures, or formulae, or even nothing explicit, nothing we can even verbalise. Case in point, Feynman was thinking with pictures, and his diagrams were helpful, but that abstraction was not a pre-existing statistical result that was extrapolated, but rather an emergent consequence of familiarity with Wick&#39;s theorem. LLMs have no room for such an abstract process, or at least, it&#39;s hard to tell. True, they <em>could</em> have this, but that is an extraordinary claim, backed by (at best) lack of any evidence to the contrary.</p><p>Too vague? Let&#39;s consider an example and assume you&#39;re making a small UI component, using HTML+CSS or some GUI library. The ML model would be able to generate the form code, but only sometimes and often semi-accurately. A week ago we, Greybeard, asked GPT-4 to write the code for a CSS-only image gallery. The code simply did not work. It was like a straight F student copying the work of a straight A student, without even remotely understanding what they were doing. It&#39;d be an automatic fail for anyone reasonable, but let&#39;s indulge the &quot;true believers&quot;.</p><p>Let&#39;s assume there&#39;s a tiniest bit of design involved: there are limited field widths and the designer decided to integrate the icons into the form fields. How would an LLM be sure that the result matches the criteria without seeing it? One could think of things like <a href="https://cliport.github.io/" target="_blank" rel="noreferrer">CLIPort</a> that was made for robotic manipulators or <a href="https://llava-vl.github.io/" target="_blank" rel="noreferrer">LLaVA</a> made to work with vision-related tasks, but modifying GPT to interact with it and to reason about the design is not the easiest task. As a human, and as someone who has worked with HTML for a very long time, given structure of the document, I can project what it would look like; I can do almost as well in my mind&#39;s eye, as the browser can in its canvas. The LLM, could in principle interface with the browser to render the results exactly, yet doesn&#39;t even &quot;think&quot; to do that. Predictably, it will often horrendously misinterpret the constraints, and sometimes ignore them completely.</p><p>Let&#39;s go further. A human can modify the page further, incrementally change the design. Can an LLM do the same? It could generate the code wholesale, but not make surgical adjustments: this would require the model finding precisely where to select the text and to then have an improved word mask model to alter the text at least slighlty more effecively than now. Using an LLM with a prompt fed to it to alter the same section will lead to multitudes of hallucination iterations to be handled, and it&#39;s not fun to handle whatsoever. The <a href="https://www.youtube.com/watch?v=RDd71IUIgpg&amp;t=311s" target="_blank" rel="noreferrer">primagean</a> demonstrated the problems in using GitHub Copilot. The LLM simply ignores some of the constraints in the video, it generated a frames-per-second where the time was measured in miliseconds. I know of some models that guess a masked word[^Bert_word-masking][^fill-mask], but doing the inverse with a set goal <strong>consistently</strong>? It&#39;s not impossible, but it may very well be tedious to tune. And maybe said models could be reused. Creating a corpus for these models is a massive work, and one should cover all edge-cases with many models. According to TIME, &quot;<a href="https://time.com/6247678/openai-chatgpt-kenya-workers/" target="_blank" rel="noreferrer">OpenAI Used Kenyan Workers on Less Than $2 Per Hour to Make ChatGPT Less Toxic</a>&quot;. Are there enough people to work on all of these tasks?</p><h2 id="minor-complaints" tabindex="-1">Minor complaints <a class="header-anchor" href="#minor-complaints" aria-label="Permalink to &quot;Minor complaints&quot;">​</a></h2><p>It gets sillier! Often enough, LLMs simply stop writing the text and you need to make them continue from that point on manually! I haven&#39;t yet seen a cover-all method that allows LLMs to automatically start and stop, GPT-4 included. Maybe GPT-5 will do that? ChatGPT in particular sometimes breaks and writes the code after the highlight, so even if one had direct API access, weaponising this to replace an engineer would be a monumental task, defeating the original intention.</p><h3 id="some-background-on-neuroscience" tabindex="-1">Some background on Neuroscience <a class="header-anchor" href="#some-background-on-neuroscience" aria-label="Permalink to &quot;Some background on Neuroscience&quot;">​</a></h3><p>Our brains <strong>remember related information</strong>, perform action selection<sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup> based on the outside context provided by our senses, while <strong>filtering inappropriate actions out</strong>. That’s quite different in comparison to the LLMs, which, in turn, <a href="https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/" target="_blank" rel="noreferrer">generate the most probable next token</a>. Besides, the modern LLMs are limited by the data provided in training dataset: they don’t retrieve new information<sup class="footnote-ref"><a href="#fn3" id="fnref3">[3]</a></sup>. We&#39;re still stuck with the machine learning methods that can&#39;t learn real-time, requiring the immense arrays of hardware to do the training. The popular ChatGPT failed to cobble up a word of a given length out of the letters I&#39;ve picked, which Python (that is considered to be slow by many) does in less than a second on my cheap laptop. Several times in a row, because I wanted to be fair towards it and repeated my test. It’s not a description for super (or human-level) intelligence, really.</p><p>Sure, you could make the argument that some vague future models might approach the problem better. I would revisit this discussion at that time, because right now, we are projecting superhuman intelligence onto a stochastic parrot. Plus, given the no-free-lunch theorem, if there ever will be an artificial general intelligence, it will <strong>have</strong> to be only partially statistical in nature. Plus, there&#39;s a good chance that by the time we have something like AGI, we will have deepened our knowledge and there&#39;s still something somewhere that the artificial intelligence does worse than a human (for one, our brains have exceptional power efficiency).</p><h3 id="ground-for-improvement" tabindex="-1">Ground for improvement <a class="header-anchor" href="#ground-for-improvement" aria-label="Permalink to &quot;Ground for improvement&quot;">​</a></h3><p>Now, let&#39;s talk about a thing to improve. LLMs need to be able to assess what they write. If an LLM writes five or seven-letter words when it’s being requested to write six-letter words, it lacks an ability for self-assesment. If it can’t plan to read code’s files and pick the one needing change, it lacks planning. Planning does not require interaction with third-party systems, but that’ll help. And yes, since your LLM isn’t typically connected to the OS in some way, it won’t interact with the project files or create a project for you. So no, LLMs won’t replace the human programmer, not yet. They would needs more parts attached. It&#39;s not all doom and gloom, many are thinking about LLMs lacking in capabilities nowadays. There’s the project <a href="https://github.com/ezelikman/parsel" target="_blank" rel="noreferrer">Parsel project</a> that partially addresses this problem. It is described as:</p><blockquote><p>A framework enabling automatic implementation and validation of complex algorithms with code LLMs, taking hierarchical function descriptions in natural language as input&quot;</p></blockquote><p>While this sounds complex, <em>Parsel</em> solves an important task: generating the code from the natural language description using constraints.</p><p>We also need to feed data to somehow provide the context. The &quot;<a href="https://github.com/keerthanpg/talktopapers/blob/master/TalkToPapers.ipynb" target="_blank" rel="noreferrer">Talk to papers</a>&quot; and &quot;<a href="https://github.com/keerthanpg/TalkToCode" target="_blank" rel="noreferrer">Talk to code</a>&quot; demos show us an important detail of the process: the use of text embeddings (vectors pointing to a message for a language model) to look up the related info. That is a small part, which would be quite important for navigating the source code of the project, although best combined with the other search algorithms.</p><p>Imagine we want our LLM to draw a form to input the bank account details. It will be able to do the basic one. It will be able to mock something using the Bootstrap CSS framework. It will not see anything, unless connected to another neural net that has such a modality. <a href="https://openai.com/blog/clip/" target="_blank" rel="noreferrer">CLIP</a> and other similar neural networks have the ability to connect text and images, often with limited resolution, and may help a bit already. The whole field advanced slightly with the <a href="https://openai.com/blog/multimodal-neurons/" target="_blank" rel="noreferrer">multimodal neurons</a> representing the concepts being located. Otherwise, I&#39;d simply say our civilization just started tinkering with multiple modalities.</p><p>Now, we’re getting to the interesting part. How does our system select actions? How does it even know what actions it can perform? Through some API bindings that allow it to work with a codebase? It’s not even close to what LLMs currently have. There are many ML solutions for selecting an action, starting with the reinforcement learning agents and finishing with the exotic ideas like animats, though. There’s even a <a href="https://say-can.github.io/" target="_blank" rel="noreferrer">SayCan</a> assistant who has this exact ability. The problem here is that RL agents would perfectly know the possible actions, while it’s more vague with the code.</p><p>And there’s much more to machine learning than any large language model had achieved! LLMs are only a small part of what&#39;s being done, and not each part is easy to understand and appreciate. We&#39;re only starting and it&#39;s naive to assume we&#39;re going to get the complete imitation of our thinking or an improvement over it this decade.</p><p><a href="https://openai.com/" target="_blank" rel="noreferrer">OpenAI</a>, the same company that created ChatGPT, made a great demo<sup class="footnote-ref"><a href="#fn4" id="fnref4">[4]</a></sup> with robots and reinforcement learning, but people outside the company don’t interact with those proprietary networks much, so the fate of this technology for now is to be seen as «fun videos on YouTube with robots playing hide and seek». That for a story, where robots learned how to use tools, something many biological species can’t do!</p><p>Everyone is talking about ChatGPT, while the same company has GPT-instruct, that can learn on a set of ideas provided and has much less limitations as a less popular product. While one thing is being polished for the public use, a thing that&#39;ll give better results is discussed less! It makes me smile when a newbie does that, but when businesses change their strategies over ChatGPT while ignoring everything that was there before it, it is simply hilarious.</p><p>It is both amusing and bemusing to think that some people even consider replacing any part of their software engineering teams with &quot;A.&quot;&quot;I.&quot;. You see, if we approach this in the straightforward way, the very people who work with ML models should be replaced through the sheer amount of data available on ML code. But does the code itself represent the whole process here? Given how much is hidden in the dataset and the model configuration, I highly doubt it. The code is not guaranteed to be straightforward and have a good architecture, it is not even guaranteed to make much sense at the first glance, yet there is a place and time for &quot;scientific style of programming&quot;, which we often see in ML. But let&#39;s not stop here and pick something much easier. Historically, code that writes code was called different names, for example, &quot;symbolic regression&quot; and &quot;genetic programming&quot;. And heck, given how much goes into picking data and tuning the genetic programming libraties, I dream about it being automated. The code is short, usually representing some visualization and a config parser. And yet, each time there&#39;s still some small trick to the data, something to optimize. LLMs won&#39;t infer formulas and won&#39;t configure the Cartesian Genetic Programming systems to make some DSP filter for sound or images soon. For now, they&#39;ll help generate the glue code.</p><h3 id="the-way-forward" tabindex="-1">The way forward <a class="header-anchor" href="#the-way-forward" aria-label="Permalink to &quot;The way forward&quot;">​</a></h3><p>Finally, the scientists are tinkering with the ideas, which may put those technologies in our homes, instead of the large research labs with massive funding.</p><p>Let’s discuss something called a <a href="https://en.wikipedia.org/wiki/Memristor" target="_blank" rel="noreferrer">memristor</a> or a memory resistor, starting from the basics. Normal resistors reduce the current flow in electronic circuits and do a lot more useful stuff by converting electric power to heat. So far it is not new, but at some point, the transistors appeared: something that acts like a resistor, but can be controlled by applying the electric power. Now, with the ability to make something complex, like logic gates, people tinkered with the technology more and more, made it smaller and smaller, integrated gates to complex circuits, and now we’ve got the powerful computers in our pockets. What crazy networks with miriads of parts can we expect from yet another «more complex resistor sibling», then? Memristors have a great potential for machine learning, because each of them has a way to store information, while resistance may be used to process it in analog way. This is quite similar to what neurons in our brains do. The progress of memristor development was partially parallel to the transistors, since the term was coined in 1971 by <a href="https://en.wikipedia.org/wiki/Leon_O._Chua" target="_blank" rel="noreferrer">Leon Chua</a>. I wanted to add one reference to a single-molecule memristor that can be inkjet-printed from an article, but now there seems to be more than one type, plus something that can be tuned by light and another, with a magnetic spin. More importantly, there&#39;s an article that tells about the on-line learning ability of the memristor networks now<sup class="footnote-ref"><a href="#fn5" id="fnref5">[5]</a></sup>. The memristors may very well provide us with an ability to train such networks at the leisure of our homes at some point in future. But for now, we&#39;ve got the disconnected ML models doing some parts of the whole we need.</p><p>Besides the training, there are other companies having impressive results, for example, <a href="https://optalysys.com/" target="_blank" rel="noreferrer">Optalysis</a>. They&#39;re using the Fourier transform caused by an optical system to immediately perform ML inference tasks. In their article, &quot;<a href="https://web.archive.org/web/20221210061657/https://optalysys.com/optical-computing-and-transformer-networks/" target="_blank" rel="noreferrer">Optalysis and Fourier-based transformers</a>&quot;, they claim that they were able to impressively accelerate the transformer inference. While it&#39;s nowhere near something necessary for training, these devices may soon be an amazing extensions for the workstations, and someday, home computers, also allowing us to run these networks locally. MythicAI had <a href="https://youtu.be/GVsUOuSjvcg?t=961" target="_blank" rel="noreferrer">demonstrated</a> a way to run ML tasks on a RAM chip, using its other properties. This can be an alternative to what Optalysis is doing with the Fourier optics.</p><h2 id="conclusion" tabindex="-1">Conclusion <a class="header-anchor" href="#conclusion" aria-label="Permalink to &quot;Conclusion&quot;">​</a></h2><p>We have demonstrated that at present, us meatbags can look forward to a new type of work, namely fixing what the LLM has generated, instead of writing it out ourselves. Human programmers will be a tad more productive, naturally this will not result in higher compensation. We live in a perverse world, and a 10x improvement in productivity won&#39;t make most software engineers 10x the pay, though it should, and under a different economic system, one the US had before 1972 it would.</p><p>The advent of LLMs will not reduce the amount of workplaces for people of the software-engineering bend. What it will result in, is you no longer having to write a dumb function to do something simple, but oversee that the function that was generated by the LLM isn&#39;t too dumb.</p>',41),m={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},u={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.581ex"},xmlns:"http://www.w3.org/2000/svg",width:"11.341ex",height:"2.151ex",role:"img",focusable:"false",viewBox:"0 -694 5012.8 950.9","aria-hidden":"true"},p=t('<g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="6C" d="M42 46H56Q95 46 103 60V68Q103 77 103 91T103 124T104 167T104 217T104 272T104 329Q104 366 104 407T104 482T104 542T103 586T103 603Q100 622 89 628T44 637H26V660Q26 683 28 683L38 684Q48 685 67 686T104 688Q121 689 141 690T171 693T182 694H185V379Q185 62 186 60Q190 52 198 49Q219 46 247 46H263V0H255L232 1Q209 2 183 2T145 3T107 3T57 1L34 0H26V46H42Z" style="stroke-width:3;"></path><path data-c="6F" d="M28 214Q28 309 93 378T250 448Q340 448 405 380T471 215Q471 120 407 55T250 -10Q153 -10 91 57T28 214ZM250 30Q372 30 372 193V225V250Q372 272 371 288T364 326T348 362T317 390T268 410Q263 411 252 411Q222 411 195 399Q152 377 139 338T126 246V226Q126 130 145 91Q177 30 250 30Z" transform="translate(278,0)" style="stroke-width:3;"></path><path data-c="67" d="M329 409Q373 453 429 453Q459 453 472 434T485 396Q485 382 476 371T449 360Q416 360 412 390Q410 404 415 411Q415 412 416 414V415Q388 412 363 393Q355 388 355 386Q355 385 359 381T368 369T379 351T388 325T392 292Q392 230 343 187T222 143Q172 143 123 171Q112 153 112 133Q112 98 138 81Q147 75 155 75T227 73Q311 72 335 67Q396 58 431 26Q470 -13 470 -72Q470 -139 392 -175Q332 -206 250 -206Q167 -206 107 -175Q29 -140 29 -75Q29 -39 50 -15T92 18L103 24Q67 55 67 108Q67 155 96 193Q52 237 52 292Q52 355 102 398T223 442Q274 442 318 416L329 409ZM299 343Q294 371 273 387T221 404Q192 404 171 388T145 343Q142 326 142 292Q142 248 149 227T179 192Q196 182 222 182Q244 182 260 189T283 207T294 227T299 242Q302 258 302 292T299 343ZM403 -75Q403 -50 389 -34T348 -11T299 -2T245 0H218Q151 0 138 -6Q118 -15 107 -34T95 -74Q95 -84 101 -97T122 -127T170 -155T250 -167Q319 -167 361 -139T403 -75Z" transform="translate(778,0)" style="stroke-width:3;"></path></g><g data-mml-node="TeXAtom" transform="translate(1311,-241.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z" style="stroke-width:3;"></path><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z" transform="translate(500,0)" style="stroke-width:3;"></path></g></g></g><g data-mml-node="mo" transform="translate(2068.1,0)"><path data-c="2061" d="" style="stroke-width:3;"></path></g><g data-mml-node="mn" transform="translate(2234.8,0)"><path data-c="33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z" style="stroke-width:3;"></path><path data-c="2E" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z" transform="translate(500,0)" style="stroke-width:3;"></path><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z" transform="translate(778,0)" style="stroke-width:3;"></path><path data-c="34" d="M462 0Q444 3 333 3Q217 3 199 0H190V46H221Q241 46 248 46T265 48T279 53T286 61Q287 63 287 115V165H28V211L179 442Q332 674 334 675Q336 677 355 677H373L379 671V211H471V165H379V114Q379 73 379 66T385 54Q393 47 442 46H471V0H462ZM293 211V545L74 212L183 211H293Z" transform="translate(1278,0)" style="stroke-width:3;"></path><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z" transform="translate(1778,0)" style="stroke-width:3;"></path><path data-c="36" d="M42 313Q42 476 123 571T303 666Q372 666 402 630T432 550Q432 525 418 510T379 495Q356 495 341 509T326 548Q326 592 373 601Q351 623 311 626Q240 626 194 566Q147 500 147 364L148 360Q153 366 156 373Q197 433 263 433H267Q313 433 348 414Q372 400 396 374T435 317Q456 268 456 210V192Q456 169 451 149Q440 90 387 34T253 -22Q225 -22 199 -14T143 16T92 75T56 172T42 313ZM257 397Q227 397 205 380T171 335T154 278T148 216Q148 133 160 97T198 39Q222 21 251 21Q302 21 329 59Q342 77 347 104T352 209Q352 289 347 316T329 361Q302 397 257 397Z" transform="translate(2278,0)" style="stroke-width:3;"></path></g></g></g>',1),g=[p],f=e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("msub",null,[e("mi",null,"log"),e("mrow",{"data-mjx-texclass":"ORD"},[e("mn",null,"10")])]),e("mo",{"data-mjx-texclass":"NONE"},"⁡"),e("mn",null,"3.1416")],-1),w=t('<hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="fn1" class="footnote-item"><p><a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noreferrer">Attention Is All You Need</a>, Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin, 2017 <a href="#fnref1" class="footnote-backref">↩︎</a></p></li><li id="fn2" class="footnote-item"><p><a href="https://compcogneuro.org/" target="_blank" rel="noreferrer">Computational Cognitive Neuroscience, 4th Edition</a> by R. C. O&#39;Reilly, Y. Munakata, M. J. Frank, T. E. Hazy, &amp; Contributors, &quot;Chapter 7: Motor Control and Reinforcement Learning&quot;, &quot;Basal Ganglia, Action Selection and Reinforcement Learning&quot; <a href="#fnref2" class="footnote-backref">↩︎</a></p></li><li id="fn3" class="footnote-item"><p>I hope that ChatGPT will use the results of the user&#39;s estimation as the training data, but we&#39;ll see. <a href="#fnref3" class="footnote-backref">↩︎</a></p></li><li id="fn4" class="footnote-item"><p><a href="https://www.youtube.com/watch?v=Lu56xVlZ40M" target="_blank" rel="noreferrer">OpenAI Plays Hide and Seek…and Breaks The Game! 🤖</a> <a href="#fnref4" class="footnote-backref">↩︎</a></p></li><li id="fn5" class="footnote-item"><p>&quot;<a href="https://asic2.group/wp-content/uploads/2017/05/TNNLS.pdf" target="_blank" rel="noreferrer">Memristor-Based Multilayer Neural Networks With Online Gradient Descent Training</a>&quot; by Daniel Soudry, Dotan Di Castro, Asaf Gal, Avinoam Kolodny, and Shahar Kvatinsky <a href="#fnref5" class="footnote-backref">↩︎</a></p></li></ol></section>',2);function b(y,k,T,v,Q,L){const i=s("mjx-assistive-mml");return n(),a("div",null,[c,e("p",null,[o('Fearmongering, and perverse incentives will make most script kiddies nervous, because what they need several hours to do, Copilot or ChatGPT will do in a fraction of a second. Guess what, there used to be a profession called "computer", where humans did computations by hand, something like figuring out what the '),e("mjx-container",m,[(n(),a("svg",u,g)),l(i,{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},{default:h(()=>[f]),_:1})]),o(" is for some logarithmic table or slide rule, the kind of work changed, but a mathematical profession needed for automation never went away. Software engineering will likely rebrand to something else, but the people with particular skills and proclivities will find a position managing the automatic tools.")]),w])}const M=r(d,[["render",b]]);export{x as __pageData,M as default};
